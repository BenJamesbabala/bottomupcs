<chapter id="wk2">
  
  <title>Computer Architecture for Beginners</title>

  <sect1>
    <title>The CPU</title>
    
    <figure>
      <title>The CPU</title>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="wk2/figures/computer.eps" format="EPS">
	</imageobject>
	<imageobject>
	  <imagedata fileref="wk2/figures/computer.png" format="PNG">
	</imageobject>
	<textobject>
	  <phrase>The CPU performs instructions on values held in
	      registers.  This example shows firstly setting the value
	      of R1 to 100, loading the value from memory location
	      0x100 into R2, adding the two values together and
	      placing the result in R3 and finally storing the new
	      value (110) to R4 (for further use). </phrase>
	</textobject>
      </mediaobject>

    </figure>

    <para>To greatly simplify, a computer consists of a central
    processing unit (CPU) attached to memory.  The figure above
    illustrates the general principle behind all computer
    operations.</para>

    <para>The CPU executes instructions read from memory.  There are
    two categories of instructions</para>

    <orderedlist>
      <listitem>
	<para>Those that <emphasis>load</emphasis> and
	<emphasis>store</emphasis> values from memory and to memory,
	respectively.</para>
      </listitem>
      <listitem>
	<para>Those that operate on values stored in
	registers.</para>
      </listitem>
    </orderedlist>

    <para>So in the example we are simply adding 100 to a value stored
    in memory, and storing this new result back into memory.</para>

    <sect2>
      <title>Branching</title>

      <para>Apart from loading or storing, the other important
	operation of a CPU is <emphasis>branching</emphasis>.
	Internally, the CPU keeps a record of the next instruction to
	be executed in the <emphasis>instruction pointer</emphasis>.
	Usually, the instruction pointer is incremented to point to the
	next instruction sequentially; the branch instruction will
	usually check if a specific register zero or if a flag is set
	and, if so, will modify the pointer to a different address.
	This is how loops and decision statements
	(i.e. <computeroutput>if</computeroutput> statements) are
	handled by the processor.</para>
    </sect2>

    <sect2>
      <title>Cycles</title>

      <para>We are all familiar with the speed of the computer, given
	in Megahertz or Gigahertz (millions or thousands of millions
	cycles per second).  This is called the <emphasis>clock
	speed</emphasis> since it is the speed that an internal clock
	within the computer pulses.</para>

      <para>The pulses are used within the processor to keep
      everything happening inside it sync.  On each pulse another
      operation can be started; think of the clock like the person
      beating the drum to keep the rower's oars in sync. </para>

    </sect2>

    <sect2>
      <title>Fetch, Decode, Execute, Store</title>

      <para>Executing an instruction consists of a particular cycle
	of events, particularly fetching, decoding, executing and
	storing.</para>

      <para>For example, to do the
	<computeroutput>add</computeroutput> instruction above it must
	be</para>

      <orderedlist>
	<listitem>
	  <para>Fetch : that is from memory into the
	    processor.</para>
	</listitem>
	<listitem>
	  <para>Decode : the processor must internally decode what
	    it has to do (in this case add).</para>
	</listitem>
	<listitem>
	  <para>Execute : take the values from the registers,
	    actually add them together</para>
	</listitem>
	<listitem>
	  <para>Store : store the result back into another
	    register.</para>
	</listitem>
      </orderedlist>

      <para>Once upon a time, all of these steps would have taken
	place during one cycle.  However as hardware engineers started
	to increase the clock speed, it became impractical to do all
	those steps in one cycle.</para>

      <para>Internally, the CPU has many different sub components that
	perform each of the above steps, and generally they can all
	happen at the same time.  The CPU can be smart and schedule
	many different parts of different operations all at the same
	time, effectively doing multiple instructions in the one clock
	cycle.  This is called
	<emphasis>pipelining</emphasis><footnote><para>In fact, any
	modern processor has many more than four stages it can
	pipeline.  The more stages that can be executed at the same
	time, the deeper the pipeline.</para></footnote>, and a
	processor that can do this is referred to as a
	<emphasis>superscalar architecture</emphasis>.  All modern
	processors are superscalar.</para>

      <para>We can think of the pipeline like a normal every day pipe
      that is being filled with marbles.  Ideally you will be putting
      your marbles in one end, one after the other (synchronised with
      clock pulses), filling up the pipe.  Once full, for each marble
      (instruction) you push in all the others will move to the next
      position and one will fall out the end (the result).</para>

      <para>Branch instruction play havoc with this model however,
	since they may or may not cause execution to start from a
	different place.  If you are pipelining, you will have to
	basically guess which way the branch will go, so you know
	which instructions to bring into the pipeline.  If the CPU has
	predicted correctly, everything goes fine! Conversely, if the
	processor has predicted incorrectly it has wasted a lot of
	time and has to clear the pipeline and start again.</para>

      <para>This process is usually referred to as a
      <emphasis>pipeline flush</emphasis> and is analogous to having
      to stop and empty out all your marbles from your hose!</para>

    </sect2>

    <sect2>
      <title>CISC v RISC</title>

      <para>A common way to divide computer architectures is into
      <emphasis>Complex Instruction Set Computer</emphasis> (CISC) and
      <emphasis>Reduced Instruction Set Computer</emphasis>
      (RISC).</para>

      <para>Note in the first example, we have explicitly loaded
      values into registers, performed an addition and stored the
      result value held in another register back to memory.  This is
      an example of a RISC approach to computing -- only performing
      operations on values in registers and explicitly loading and
      storing values to and from memory.
      </para>

      <para>A CISC approach would use only a single instruction taking
      values from memory, performing the addition internally and
      writing the result back.  This means the instruction may take
      many cycles, but ultimately both approaches achieve the same
      goal.</para>

      <para>All modern architectures would be considered RISC
      architectures<footnote><para>Even the most common architecture,
      the Intel Pentium, whilst having an instruction set that is
      categorised as CISC, internally breaks down instructions to RISC
      style sub-instructions inside the chip before
      executing.</footnote>.</para>

      <para>There are a number of reasons for this</para>

      <itemizedlist>
	<listitem>
	  <para>Whilst RISC makes assembly programming becomes more
	  complex, since virtually all programmers use high level
	  languages and leave the hard work of producing assembly
	  code to the compiler, so the other advantages outweigh this
	  disadvantage.</para>
	</listitem>

	<listitem>
	  <para>Because the instructions in a RISC processor are much
	  more simple, there is more space inside the chip for
	  registers.  As we know from the memory hierarchy, registers
	  are the fastest type of memory and ultimately all
	  instructions must be performed on values held in registers,
	  so all other things being equal more registers leads to
	  higher performance.</para>
	</listitem>

	<listitem>
	  <para>Since all instructions execute in the same time,
	  pipelining is possible.  We know pipelining requires streams
	  of instructions being constantly fed into the processor, so
	  if some instructions take a very long time and others do
	  not, the pipeline becomes far to complex to be
	  effective.</para>
	</listitem>
      </itemizedlist>

      <sect3>
	<title>EPIC</title>

	<para>The Itanium processor, which is used in many example
	through this book, is an example of a modified architecture
	called Explictily Parallel Instruction Computing.</para>

	<para>We have discussed how superscaler processors have
	pipelines that have many instructions in flight at the same
	time in different parts of the processor.  Obviously for this
	to work as well as possible instructions should be given the
	processor in an order that can make best use of the
	available elements of the CPU.</para>

	<para>Traditionally organising the incoming instruction stream
	has been the job of the hardware.  Instructions are issued by
	the program in a sequental manner; the processor must look
	ahead and try to make decisions about how to organise the
	incoming instructions.</para>

	<para>The theory behind EPIC is that there is more information
	available at higher levels which can make these decisions
	better than the processor.  Analysing a stream of assebembly
	language instructions, as current processors do, looses a lot
	of information that the programmer may have provided in the
	original source code.  Think of it as the difference between
	studying a Shakespeare play and reading the Cliff's Notes
	version of the same.  Both give you the same result, but the
	orignal has all sorts of pehperial information that sets the
	scene and gives you insight into the characters.</para>

	<para>Thus the logic of ordering instructions can be moved
	from the processor to the compiler.  This means that compiler
	writers need to be smarter to try and find the best ordering
	of code for the processor.  The processor is also
	significantly simplified, since a lot of its work has been
	moved to the compiler.<footnote> <para>Another term often used
	around EPIC is Very Long Instruction World (VLIW), which is
	where each instruction to the processor is extended to tell
	the processor about where it should execute the instruction in
	it's internal units.  The problem with this approach is that
	code is then completly dependent on the model of processor is
	has been compiled for.  Companies are always making revisions
	to hardware, and making customers recompile their application
	every single time, and maintain a range of different binaries
	was impractical.</para> <para>EPIC solves this in the usual
	computer science manner by adding a layer of abstraction.
	Rather than explicitly specifying the exact part of the
	processor the instructions should exectue on, EPIC creates a
	simplified view with a few core units like memory, integer and
	floating point.</para></footnote></para>

      </sect3>

    </sect2>

  </sect1>

  <sect1>
    <title>Memory</title>

    <sect2>
      <title>Memory Hierarchy</title>

      <para>The CPU can only directly fetch instructions and data
	from cache memory, located directly on the processor chip.
	Cache memory must be loaded in from the main system memory
	(the Random Access Memory, or RAM).  RAM however, only retains
	it's contents when the power is on, so needs to be stored on
	more permanent storage.</para>

      <para>We call these layers of memory the <emphasis>memory
      hierarchy</emphasis></para>

      <table>
	<title>Memory Hierarchy</title>
	<tgroup cols="3">
	  <thead>
	    <row>
	      <entry>Speed</entry>
	      <entry>Memory</entry>
	      <entry>Description</entry>
	    </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry>Fastest</entry>
	      <entry>Cache</entry>
	      <entry>Cache memory is memory actually embedded inside
		the CPU.  Cache memory is very fast, typically taking
		only once cycle to access, but since it is embedded
		directly into the CPU there is a limit to how big it
		can be.  In fact, there are several sub-levels of
		cache memory (termed L1, L2, L3) all with slightly
		increasing speeds.</entry>
	    </row>

	    <row>
	      <entry></entry> 
	      <entry>RAM</entry> 
	      <entry>All instructions and storage addresses for the
		processor must come from RAM.  Although RAM is very
		fast, there is still some significant time taken for
		the CPU to access it (this is termed
		<emphasis>latency</emphasis>). RAM is stored in
		separate, dedicated chips attached to the motherboard,
		meaning it is much larger than cache memory.
		</entry>
	    </row>

	      <row>
		<entry>Slowest</entry>
		<entry>Disk</entry>
		<entry>We are all familiar with software arriving on a
		floppy disk or CDROM, and saving our files to the hard
		disk.  We are also familiar with the long time a
		program can take to load from the hard disk -- having
		physical mechanisms such as spinning disks and moving
		heads means disks are the slowest form of storage.
		But they are also by far the largest form of
		storage.</entry>
	      </row>
	    </tbody>
	  </tgroup>
	</table>

	<para>The important point to know about the memory hierarchy
	is the trade offs between speed an size -- the faster the
	memory the smaller it is.  Of course, if you can find a way to
	change this equation, you'll end up a billionaire!</para>

      <sect3>
	<title>Cache in depth</title>

	<para>Cache is one of the most important elements of the CPU
	  architecture.  To write efficient code developers need to
	  have an understanding of how the cache in their systems
	  works.</para>

	<para>l1, l2, l3.  cache line size.  example of thrashing
	cache. write-through v write-back.  </para>

      </sect3>

      </sect2>

    </sect1>

  <sect1>
    <title>Peripherals and busses</title>

    <para>Peripherals are any of the many external devices that
      connect to your computer.  Obviously, the processor must have
      some way of talking to the peripherals to make them
      useful.</para>
    
    <para>The communication channel between the processor and the
      peripherals is called a <emphasis>bus</emphasis>.  The devices
      directly connected to the processor use a type of bus called
      Peripheral Component Interconnect, commonly referred to as
      PCI.</para>
    
    <sect2>
      <title>PCI Bus</title>

      <para>PCI transfers data between the device and memory, but
      importantly allows for the automatic configuration of attached
      peripherals.  The configuration broadly falls into two categories</para>
      
      <sect3>
	<title>Interrupts</title>
	  
	<para>An interrupt allows the device to literally interrupt
	  the processor to flag some information.  For example, when a
	  key is pressed, an interrupt is generated and delivered to
	  the CPU.  An interrupt (called the IRQ) is assigned to the
	  device on system boot by the system BIOS.</para>
	
	<para>When the device wants to interrupt, it will signal to
	the processor via raising the voltage of <emphasis>interrupt
	pins</emphasis>.  The processor will acknowledge the
	interrupt, and pass the IRQ onto the operating system.  This
	part of the operating system code is called the
	<emphasis>interrupt handler</emphasis>.</para>
	
	<para>The interrupt handler knows what to do with the
	interrupt as when each device driver initialises it will
	register its self with the kernel to accept the interrupt from
	the peripheral it is written for.  So as the interrupt
	arrives it is passed to the driver which can deal with the
	information from the device correctly.</para>
	
	<para>Most drivers will spilt up handling of interrupts into
	<emphasis>bottom</emphasis> and <emphasis>top</emphasis>
	halves.  The bottom half will acknowledge the interrupt and
	return the processor to what it was doing quickly.  The top
	half will then run later when the CPU is free and do the more
	intensive processing.  This is to stop an interrupt hogging
	the entire CPU.</para>
	
      </sect3>
	
      <sect3>
	<title>IO Space</title>
	  
	<para>Obviously the processor will need to communicate with
	the peripheral device, and it does this via IO operations.
	The most common form of IO is so called <emphasis>memory
	mapped IO</emphasis> where registers on the device are
	<emphasis>mapped</emphasis> into memory.</para>
	
	<para>This means that to communicate with the device, you need
	simply read or write to a specific address in memory.  TODO:
	expand</para>
	
      </sect3>

    </sect2>
    
    <sect2>
      <title>DMA</title>

      <para>Since the speed of devices is far below the speed of processors,
    there needs to be some way to avoid making the CPU wait around
    for data from devices.</para>
      
      
      <para>Direct Memory Access (DMA) is a method of transferring
	data directly between an peripheral and system RAM.</para>
      
      <para>The driver can setup a device to do a DMA transfer by
	giving it the area of RAM to put it's data into.  It can then
	start the DMA transfer and allow the CPU to continue with
	other tasks.</para>
      
      <para>Once the device is finished, it will raise an interrupt
	and signal to the driver the transfer is complete.  From this
	time the data from the device (say a file from a disk, or
	frames from a video capture card) is in memory and ready to be
	used.</para>

    </sect2>


    <sect2>
      <title>Other Busses</title>

    <para>Other busses connect between the PCI bus and external
    devices.  Some you will have heard of</para>

    <itemizedlist>
      <listitem>
	<para>USB/Firewire for small external data devices.</para>
      </listitem>
      <listitem>
	<para>IDE/SCSI for disk drives.</para>
      </listitem>
    </itemizedlist>
  </sect2>

  </sect1>

    <sect1>
      <title>Exercises</title>
      <itemizedlist>
	<listitem>
	  <para>Explain why the clock speed, given in cycles per
	  second, is not the best indicator of actual processor
	  speed.</para>
	</listitem>
      </itemizedlist>
    </sect1>

</chapter>

<!--
Local Variables:
mode: sgml
sgml-parent-document: ("../csbu.sgml" "book" "chapter")
End:
-->