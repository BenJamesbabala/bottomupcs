<chapter id="wk2">
  
  <title>Computer Architecture for Beginners</title>

  <sect1>
    <title>The CPU</title>
    
    <figure>
      <title>The CPU</title>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="wk2/figures/computer.eps" format="EPS">
	</imageobject>
	<imageobject>
	  <imagedata fileref="wk2/figures/computer.png" format="PNG">
	</imageobject>
	<textobject>
	  <phrase>The CPU performs instructions on values held in
	      registers.  This example shows firstly setting the value
	      of R1 to 100, loading the value from memory location
	      0x100 into R2, adding the two values together and
	      placing the result in R3 and finally storing the new
	      value (110) to R4 (for further use). </phrase>
	</textobject>
      </mediaobject>

    </figure>

    <para>To greatly simplify, a computer consists of a central
    processing unit (CPU) attached to memory.  The figure above
    illustrates the general principle behind all computer
    operations.</para>

    <para>The CPU executes instructions read from memory.  There are
    two categories of instructions</para>

    <orderedlist>
      <listitem>
	<para>Those that <emphasis>load</emphasis> values from memory
	into registers and <emphasis>store</emphasis> values from
	registers to memory.</para>
      </listitem>
      <listitem>
	<para>Those that operate on values stored in registers.  For
	example adding, subtracting multiplying or dividing the values
	in two registers, performing bitwise operations (and, or, xor,
	etc) or performing other mathematical operations (square root,
	sin, cos, tan, etc). </para>
      </listitem>
    </orderedlist>

    <para>So in the example we are simply adding 100 to a value stored
    in memory, and storing this new result back into memory.</para>

    <sect2>
      <title>Branching</title>

      <para>Apart from loading or storing, the other important
	operation of a CPU is <emphasis>branching</emphasis>.
	Internally, the CPU keeps a record of the next instruction to
	be executed in the <emphasis>instruction pointer</emphasis>.
	Usually, the instruction pointer is incremented to point to
	the next instruction sequentially; the branch instruction will
	usually check if a specific register is zero or if a flag is
	set and, if so, will modify the pointer to a different
	address.  Thus the next instruction to execute will be from a
	different part of program; this is how loops and decision
	statements work.</para>

      <para>For example, a statement like <computeroutput>if
      (x==0)</computeroutput> might be implemented by finding the
      <computeroutput>or</computeroutput> of two registers, one
      holding <computeroutput>x</computeroutput> and the other zero;
      if the result is zero the comparison is true (i.e. all bits of
      <computeroutput>x</computeroutput> were zero) and the body of
      the statement should be taken, otherwise branch past the body
      code.</para>

    </sect2>

    <sect2>
      <title>Cycles</title>

      <para>We are all familiar with the speed of the computer, given
	in Megahertz or Gigahertz (millions or thousands of millions
	cycles per second).  This is called the <emphasis>clock
	speed</emphasis> since it is the speed that an internal clock
	within the computer pulses.</para>

      <para>The pulses are used within the processor to keep it
      internally synchronised.  On each tick or pulse another
      operation can be started; think of the clock like the person
      beating the drum to keep the rower's oars in sync. </para>

    </sect2>

    <sect2>
      <title>Fetch, Decode, Execute, Store</title>

      <para>Executing a single instruction consists of a particular
	cycle of events; fetching, decoding, executing and
	storing.</para>

      <para>For example, to do the
	<computeroutput>add</computeroutput> instruction above the CPU
	must</para>

      <orderedlist>
	<listitem>
	  <para>Fetch : get the instruction from memory into the
	  processor.</para>
	</listitem>
	<listitem>
	  <para>Decode : internally decode what it has to do (in this
	    case add).</para>
	</listitem>
	<listitem>
	  <para>Execute : take the values from the registers, actually
	  add them together</para>
	</listitem>
	<listitem>
	  <para>Store : store the result back into another
	  register.</para>
	</listitem>
      </orderedlist>

      <para>Once upon a time, all of these steps would have taken
	place during one cycle.  However as hardware engineers started
	to increase the clock speed, it became impractical to do all
	those steps in one cycle.</para>

      <para>Internally the CPU has many different sub components that
	perform each of the above steps, and generally they can all
	happen independently of each other.  This is analgous to a
	physical production line, where there are many stations where
	each step has a particular task to perform.  Once done it can
	pass the results to the next station and take a new input to
	work on.</para>

      <para>In the same way the CPU can be smart and schedule many
	different parts of different operations all at the same time
	in different parts of the CPU.  For example, the logic for
	adding two registers together is completly separate to the
	logic for writing the result back to memory, so there is no
	reason why the CPU can not be doing both at once; effectively
	doing multiple instructions in the one clock cycle.  This is
	called <emphasis>pipelining</emphasis><footnote><para>In fact,
	any modern processor has many more than four stages it can
	pipeline.  The more stages that can be executed at the same
	time, the deeper the pipeline.</para></footnote>, and a
	processor that can do this is referred to as a
	<emphasis>superscalar architecture</emphasis>.  All modern
	processors are superscalar.</para>

      <para>Another analogy might be to think of the pipeline like a
      hose that is being filled with marbles.  Ideally you will be
      putting your marbles in one end, one after the other (one per
      clock pulse), filling up the pipe.  Once full, for each marble
      (instruction) you push in all the others will move to the next
      position and one will fall out the end (the result).</para>

      <para>Branch instruction play havoc with this model however,
	since they may or may not cause execution to start from a
	different place.  If you are pipelining, you will have to
	basically guess which way the branch will go, so you know
	which instructions to bring into the pipeline.  If the CPU has
	predicted correctly, everything goes fine! Conversely, if the
	processor has predicted incorrectly it has wasted a lot of
	time and has to clear the pipeline and start again.</para>

      <para>This process is usually referred to as a
      <emphasis>pipeline flush</emphasis> and is analogous to having
      to stop and empty out all your marbles from your hose!</para>

      <sect3>
	<title>Reordering</title>

	<para>In fact, if the CPU is the hose, it is free to reorder
	the marbles within the hose, as long as they pop out the end
	in the same order you put them in.  We call this
	<emphasis>program order</emphasis> since this is the order
	that instructions are given in the computer program.</para>

	<para>Most modern CPUs will have some way to reorder the
	instructions inside the pipeline to make most efficient use of
	resources.  For the most part you do not need to worry about
	how this works, as the CPU ensures what comes out the other
	end is in "program order".</para>

	<para>However, when writing very low level code some
	instructions may require some security about how operations
	are ordered.  We call this requirement <emphasis>memory
	semantics</emphasis>.  If you require
	<emphasis>acquire</emphasis> semantics this means that for
	this instruction you must ensure that the results of all
	previous instructions have been completed.  If you require
	<emphasis>release</emphasis> semantics you are saying that all
	instructions after this one must see the current result.
	Another even stricter semantic is a <emphasis>memory
	barrier</emphasis> or <emphasis>memory fence</emphasis> which
	requires that operations have been committed to memory before
	continuing.</para>

	<para>On some architectures these semantics are guarunteed for
	you by the processor, whilst on others you must specify them
	explicitly.  Most programmers do not need to worry directly
	about them, although you may see the terms.</para>

      </sect3>

    </sect2>

    <sect2>
      <title>CISC v RISC</title>

      <para>A common way to divide computer architectures is into
      <emphasis>Complex Instruction Set Computer</emphasis> (CISC) and
      <emphasis>Reduced Instruction Set Computer</emphasis>
      (RISC).</para>

      <para>Note in the first example, we have explicitly loaded
      values into registers, performed an addition and stored the
      result value held in another register back to memory.  This is
      an example of a RISC approach to computing -- only performing
      operations on values in registers and explicitly loading and
      storing values to and from memory.</para>

      <para>A CISC approach may only a single instruction taking
      values from memory, performing the addition internally and
      writing the result back.  This means the instruction may take
      many cycles, but ultimately both approaches achieve the same
      goal.</para>

      <para>All modern architectures would be considered RISC
      architectures<footnote><para>Even the most common architecture,
      the Intel Pentium, whilst having an instruction set that is
      categorised as CISC, internally breaks down instructions to RISC
      style sub-instructions inside the chip before
      executing.</footnote>.</para>

      <para>There are a number of reasons for this</para>

      <itemizedlist>
	<listitem>
	  <para>Whilst RISC makes assembly programming becomes more
	  complex, since virtually all programmers use high level
	  languages and leave the hard work of producing assembly
	  code to the compiler, so the other advantages outweigh this
	  disadvantage.</para>
	</listitem>

	<listitem>
	  <para>Because the instructions in a RISC processor are much
	  more simple, there is more space inside the chip for
	  registers.  As we know from the memory hierarchy, registers
	  are the fastest type of memory and ultimately all
	  instructions must be performed on values held in registers,
	  so all other things being equal more registers leads to
	  higher performance.</para>
	</listitem>

	<listitem>
	  <para>Since all instructions execute in the same time,
	  pipelining is possible.  We know pipelining requires streams
	  of instructions being constantly fed into the processor, so
	  if some instructions take a very long time and others do
	  not, the pipeline becomes far to complex to be
	  effective.</para>
	</listitem>
      </itemizedlist>

      <sect3>
	<title>EPIC</title>

	<para>The Itanium processor, which is used in many example
	through this book, is an example of a modified architecture
	called Explictily Parallel Instruction Computing.</para>

	<para>We have discussed how superscaler processors have
	pipelines that have many instructions in flight at the same
	time in different parts of the processor.  Obviously for this
	to work as well as possible instructions should be given the
	processor in an order that can make best use of the
	available elements of the CPU.</para>

	<para>Traditionally organising the incoming instruction stream
	has been the job of the hardware.  Instructions are issued by
	the program in a sequental manner; the processor must look
	ahead and try to make decisions about how to organise the
	incoming instructions.</para>

	<para>The theory behind EPIC is that there is more information
	available at higher levels which can make these decisions
	better than the processor.  Analysing a stream of assebembly
	language instructions, as current processors do, looses a lot
	of information that the programmer may have provided in the
	original source code.  Think of it as the difference between
	studying a Shakespeare play and reading the Cliff's Notes
	version of the same.  Both give you the same result, but the
	orignal has all sorts of pehperial information that sets the
	scene and gives you insight into the characters.</para>

	<para>Thus the logic of ordering instructions can be moved
	from the processor to the compiler.  This means that compiler
	writers need to be smarter to try and find the best ordering
	of code for the processor.  The processor is also
	significantly simplified, since a lot of its work has been
	moved to the compiler.<footnote> <para>Another term often used
	around EPIC is Very Long Instruction World (VLIW), which is
	where each instruction to the processor is extended to tell
	the processor about where it should execute the instruction in
	it's internal units.  The problem with this approach is that
	code is then completly dependent on the model of processor is
	has been compiled for.  Companies are always making revisions
	to hardware, and making customers recompile their application
	every single time, and maintain a range of different binaries
	was impractical.</para> <para>EPIC solves this in the usual
	computer science manner by adding a layer of abstraction.
	Rather than explicitly specifying the exact part of the
	processor the instructions should exectue on, EPIC creates a
	simplified view with a few core units like memory, integer and
	floating point.</para></footnote></para>

      </sect3>

    </sect2>

  </sect1>

  <sect1>
    <title>Memory</title>

    <sect2>
      <title>Memory Hierarchy</title>

      <para>The CPU can only directly fetch instructions and data
	from cache memory, located directly on the processor chip.
	Cache memory must be loaded in from the main system memory
	(the Random Access Memory, or RAM).  RAM however, only retains
	it's contents when the power is on, so needs to be stored on
	more permanent storage.</para>

      <para>We call these layers of memory the <emphasis>memory
      hierarchy</emphasis></para>

      <table>
	<title>Memory Hierarchy</title>
	<tgroup cols="3">
	  <thead>
	    <row>
	      <entry>Speed</entry>
	      <entry>Memory</entry>
	      <entry>Description</entry>
	    </row>
	  </thead>
	  <tbody>
	    <row>
	      <entry>Fastest</entry>
	      <entry>Cache</entry>
	      <entry>Cache memory is memory actually embedded inside
		the CPU.  Cache memory is very fast, typically taking
		only once cycle to access, but since it is embedded
		directly into the CPU there is a limit to how big it
		can be.  In fact, there are several sub-levels of
		cache memory (termed L1, L2, L3) all with slightly
		increasing speeds.</entry>
	    </row>

	    <row>
	      <entry></entry> 
	      <entry>RAM</entry> 
	      <entry>All instructions and storage addresses for the
		processor must come from RAM.  Although RAM is very
		fast, there is still some significant time taken for
		the CPU to access it (this is termed
		<emphasis>latency</emphasis>). RAM is stored in
		separate, dedicated chips attached to the motherboard,
		meaning it is much larger than cache memory.
		</entry>
	    </row>

	      <row>
		<entry>Slowest</entry>
		<entry>Disk</entry>
		<entry>We are all familiar with software arriving on a
		floppy disk or CDROM, and saving our files to the hard
		disk.  We are also familiar with the long time a
		program can take to load from the hard disk -- having
		physical mechanisms such as spinning disks and moving
		heads means disks are the slowest form of storage.
		But they are also by far the largest form of
		storage.</entry>
	      </row>
	    </tbody>
	  </tgroup>
	</table>

	<para>The important point to know about the memory hierarchy
	is the trade offs between speed an size -- the faster the
	memory the smaller it is.  Of course, if you can find a way to
	change this equation, you'll end up a billionaire!</para>

      <sect3>
	<title>Cache in depth</title>

	<para>Cache is one of the most important elements of the CPU
	  architecture.  To write efficient code developers need to
	  have an understanding of how the cache in their systems
	  works.</para>

	<para>The cache is a very fast copy of the slower main system
	memory.  Cache is much smaller than main memories because it
	is included inside the processor chip alongside the registers
	and proessor logic.  This is prime realestate in computing
	terms, and there are both economic and physical limits to it's
	maximum size.  As manufacturers find more and more ways to
	cram more and more transistors onto a chip cache sizes grow
	considerably, but even the largest caches are tens of
	megabytes, rather than the gigabytes of main memory or
	terrabytes of harddisk otherwise common. (XXXexample)</para>

	<para>The cache is made up of small chunks of mirrored main
	memory.  The size of these chunks is called the <emphasis>line
	size</emphasis>, and is typically something like 64 kilobytes.
	When talking about cache, it is very common to talk about the
	line size, or a cache line, which refers to one chunk of
	mirrored main memory.  The cache can only load and store
	memory in sizes a multiple of a cache line.</para>

	<para>As the cache is quite small compared to main memory, it
	will obviously fill up quite quickly as a process goes about
	its execution.  Once the cache is full the processor needs to
	get rid of a line to make room for a new line.  There are many
	algorithms by which the processor can choose which line to
	evict; for example <emphasis>least recently used</emphasis>
	(LRU) is an algorithm where the oldest unused line is
	discarded to make room for the new line.</para>

	<para>When data is only read from the cache there is no need
	to ensure consistency with main memory.  However, when the
	processor starts writing to cache lines it needs to make some
	decisions about how to update the underlying main memory.  A
	<emphasis>write-through</emphasis> cache will write the
	changes directly into the main system memory as the processor
	updates the cache.  This is slower since the process of
	writing to the main memory is, as we have seen, slower.
	Alternatively a <emphasis>write-back</emphasis> cache delays
	writing the changes to RAM until absoultely necessary.  The
	obvious advatantage is that less main memory access is
	required when cache entries are written.  Cache lines that
	have been written but not comitted to memory are referred to
	as <emphasis>dirty</emphasis>.  The disadvantage is that when
	a cache entry is evicted, it may require two memory accesses
	(one to write dirty data main memory, and another to load the
	new data).</para>

	<para>XXX: associativity, thrashing?</para>

      </sect3>

      </sect2>

    </sect1>

  <sect1>
    <title>Peripherals and busses</title>

    <para>Peripherals are any of the many external devices that
      connect to your computer.  Obviously, the processor must have
      some way of talking to the peripherals to make them
      useful.</para>
    
    <para>The communication channel between the processor and the
      peripherals is called a <emphasis>bus</emphasis>.  The devices
      directly connected to the processor use a type of bus called
      Peripheral Component Interconnect, commonly referred to as
      PCI.</para>
    
    <sect2>
      <title>PCI Bus</title>

      <para>PCI transfers data between the device and memory, but
      importantly allows for the automatic configuration of attached
      peripherals.  The configuration broadly falls into two categories</para>
      
      <sect3>
	<title>Interrupts</title>
	  
	<para>An interrupt allows the device to literally interrupt
	  the processor to flag some information.  For example, when a
	  key is pressed, an interrupt is generated and delivered to
	  the CPU.  An interrupt (called the IRQ) is assigned to the
	  device on system boot by the system BIOS.</para>
	
	<para>When the device wants to interrupt, it will signal to
	the processor via raising the voltage of <emphasis>interrupt
	pins</emphasis>.  The processor will acknowledge the
	interrupt, and pass the IRQ onto the operating system.  This
	part of the operating system code is called the
	<emphasis>interrupt handler</emphasis>.</para>
	
	<para>The interrupt handler knows what to do with the
	interrupt as when each device driver initialises it will
	register its self with the kernel to accept the interrupt from
	the peripheral it is written for.  So as the interrupt
	arrives it is passed to the driver which can deal with the
	information from the device correctly.</para>
	
	<para>Most drivers will spilt up handling of interrupts into
	<emphasis>bottom</emphasis> and <emphasis>top</emphasis>
	halves.  The bottom half will acknowledge the interrupt and
	return the processor to what it was doing quickly.  The top
	half will then run later when the CPU is free and do the more
	intensive processing.  This is to stop an interrupt hogging
	the entire CPU.</para>
	
      </sect3>
	
      <sect3>
	<title>IO Space</title>
	  
	<para>Obviously the processor will need to communicate with
	the peripheral device, and it does this via IO operations.
	The most common form of IO is so called <emphasis>memory
	mapped IO</emphasis> where registers on the device are
	<emphasis>mapped</emphasis> into memory.</para>
	
	<para>This means that to communicate with the device, you need
	simply read or write to a specific address in memory.  TODO:
	expand</para>
	
      </sect3>

    </sect2>
    
    <sect2>
      <title>DMA</title>

      <para>Since the speed of devices is far below the speed of processors,
    there needs to be some way to avoid making the CPU wait around
    for data from devices.</para>
      
      
      <para>Direct Memory Access (DMA) is a method of transferring
	data directly between an peripheral and system RAM.</para>
      
      <para>The driver can setup a device to do a DMA transfer by
	giving it the area of RAM to put it's data into.  It can then
	start the DMA transfer and allow the CPU to continue with
	other tasks.</para>
      
      <para>Once the device is finished, it will raise an interrupt
	and signal to the driver the transfer is complete.  From this
	time the data from the device (say a file from a disk, or
	frames from a video capture card) is in memory and ready to be
	used.</para>

    </sect2>


    <sect2>
      <title>Other Busses</title>

    <para>Other busses connect between the PCI bus and external
    devices.  Some you will have heard of</para>

    <itemizedlist>
      <listitem>
	<para>USB/Firewire for small external data devices.</para>
      </listitem>
      <listitem>
	<para>IDE/SCSI for disk drives.</para>
      </listitem>
    </itemizedlist>
  </sect2>
  </sect1>
  
  <sect1>
    <title>Small to big systems</title>
    
    <para>As Moore's law has prediced, computing power has been
    growing at a furious pace and shows no signs of slowing down.  It
    is realtively uncommon for any high end servers to contain only a
    single CPU.  This is achieved in a number of different
    fashions.</para>
    
    <sect2>
      <title>Symmetric Multi-Processing</title>
      
      <para>Symmetric Multi-Processing, commonly shortended to
      <emphasis>SMP</emphasis>, is currently the most common
      configuration for including multiple CPUs in a single
      system.</para>

      <para>The symmetric term refers to the fact that all the CPUs in
      the system are the same (e.g. architecture, clock speed).  In a
      SMP system there are multiple processors that share other all
      other system resources (memory, disk, etc).</para>

      <sect3>
	<title>Cache Coherency</title>


	<para>For the most part, the CPUs in the system work
      independently; each has its own set of registers, program
      counter, etc.  Despite running separately, there is one
      component that requires strict syncronisation.</para>

      <para>This is the CPU cache; remember the cache is a small area
      of quickly accessable memory that mirrors values stored in main
      system memory.  If one CPU modifies data in main memory and
      another CPU has an old copy of that memory in its cache the
      system will obviously not be in a consistent state.  Note that
      the problem only occurs when processors are writing to memory,
      since if a value is only read the data will be
      consistent.</para>

	<para>To co-ordinate keeping the cache coherent on all
	processors an SMP system uses <emphasis>snooping</emphasis>.
	Snooping is where a processor listens on a bus which all
	processors are connected to for cache events, and updates its
	cache accordingly.</para>

	<para>One protocol for doing this is the
	<emphasis>MOESI</emphasis> protocol; standing for Modified,
	Owner, Exclusive, Shared, Invalid.  Each of these is a state
	that a cache line can be in on a processor in the system.
	There are other protocols for doing as much, however they all
	share similar concepts.  Below we examine MOESI so you have an
	idea of what the process entails.</para>

	<para>When a processor requires reading a cache line from main
	memory, it firstly has to snoop all other processors in the
	system to see if they currently know anything about that area
	of memory (e.g. have it cached).  If it does not exist in any
	other process, then the processor can load the memory into
	cache and mark it as <emphasis>exclusive</emphasis>.  When it
	writes to the cache, it then changes state to be
	<emphasis>modified</emphasis>.  Here the specific details of
	the cache come into play; some caches will immediately write
	back the modified cache to system memory (known as a
	<emphasis>write-through</emphasis> cache, because writes go
	through to main memory).  Others will not, and leave the
	modified value only in the cache until it is evicted, when the
	cache becomes full for example.</para>

	<para>The other case is where the processor snoops and finds
	that the value is in another processors cache.  If this value
	has already been marked as <emphasis>modified</emphasis>, it
	will copy the data into its own cache and mark it as
	<emphasis>shared</emphasis>.  It will send a message for the
	other processor (that we got the data from) to mark its cache
	line as <emphasis>owner</emphasis>.  Now imagine that a third
	processor in the system wants to use that memory too.  It will
	snoop and find both a <emphasis>shared</emphasis> and a
	<emphasis>owner</emphasis> copy; it will thus take its value
	from the <emphasis>owner</emphasis> value.  While all the
	other processors are only reading the value, the cache line
	stays <emphasis>shared</emphasis> in the system.  However,
	when one processor needs to update the value it sends an
	<emphasis>invalidate</emphasis> message through the system.
	Any processors with that cache line must then mark it as
	invalid, because it not longer reflects the "true" value.
	When the processor sends the invalidate message, it marks the
	cache line as <emphasis>modified</emphasis> in its cache and
	all others will mark as <emphasis>invalid</emphasis> (note
	that if the cache line is <emphasis>exclusive</emphasis> the
	processor knows that no other processor is depending on it so
	can avoid sending an invalidate message).</para>

	<para>From this point the process starts all over.  Thus
	whichever processor has the <emphasis>modified</emphasis>
	value has the responsiblity of writing the true value back to
	RAM when it is evicted from the cache.  By thinking through
	the protocol you can see that this ensures consistency of
	cache lines between processors.</para>

	<para>The only problem with this system is that all the snoop
	traffic does not scale very well.  With only a few processors,
	the overhead of checking if another processor has the cache
	line (a read snoop) or invalidating the data in every other
	processor (invalidate snoop) is managable; but as the number
	of processors increase so does the bus traffic.  This is why
	SMP systems usually only scale up to around 8
	processors.</para>

	<para>Note that system software usually has no part in this
	process, although programmers should be aware of what the
	hardware is doing underneath in response to the programs they
	design to maximise performance.</para>

      </sect3>

      <sect3>
	<title>Hyperthreading</title>

	<para>Discuss how two registers, but shared logic.</para>

      </sect3>

      <sect3>
	<title>Dual Core</title>

	<para>talk about dual cores, note that memory bus is
	shared.</para>

      </sect3>

      <sect3>
	<title>Clusters</title>

	<para>talk about clusters, slower interconnects, software
	shared memory.</para>

      </sect3>

      <sect3>
	<title>Non-Uniform Memory Access</title>

	<para>Talk about numa, mention ccNUMA</para>
      </sect3>

    </sect2>

  </sect1>

    <sect1>
      <title>Exercises</title>
      <itemizedlist>
	<listitem>
	  <para>Explain why the clock speed, given in cycles per
	  second, is not the best indicator of actual processor
	  speed.</para>
	</listitem>
      </itemizedlist>
    </sect1>

</chapter>

<!--
Local Variables:
mode: sgml
sgml-parent-document: ("../csbu.sgml" "book" "chapter")
End:
-->